{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d333019",
   "metadata": {},
   "source": [
    "\n",
    "# Clase con Laboratorio: Apps con Streamlit + PostgreSQL + Servicios de Transformación de Datos (DTS)\n",
    "**Fecha de elaboración:** 2025-10-16  \n",
    "**Curso:** (supuesto) Programación y Datos Aplicados (8 estudiantes, modalidad presencial, laboratorio computacional)  \n",
    "**Duración:** 4–5 horas pedagógicas de 45 min (180–225 min). A continuación se planifica para **225 min** con opción de ajuste a 180 min (ver notas).  \n",
    "**Supuestos declarados (en ausencia de programa formal):**\n",
    "- Resultados de aprendizaje del curso: desarrollar aplicaciones web ligeras para visualización de datos, integrar una base de datos relacional (**PostgreSQL**) y aplicar buenas prácticas de calidad de datos.\n",
    "- Contenidos previos vistos: Python básico, manejo de paquetes, nociones de HTTP/cliente-servidor, y manipulación de datos con `pandas`.\n",
    "- Restricciones logísticas: laboratorio con computadores con Python 3.10+, `pip`, permisos de instalación, conectividad estable; servidor PostgreSQL local o gestionado (p. ej., en campus o cloud).\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje (vinculados al programa)\n",
    "1. **Diseñar y desplegar** una aplicación mínima en **Streamlit** que consulte y muestre datos desde **PostgreSQL**.  \n",
    "2. **Implementar un flujo sencillo de DTS** (extracción → transformación → carga) desde un CSV a PostgreSQL, documentando supuestos y decisiones de limpieza.  \n",
    "3. **Asegurar la calidad de datos** mediante validaciones básicas (esquema, tipos, nulos, claves) y **buenas prácticas de integridad académica** (registro de fuentes y versión del código).  \n",
    "4. **Aplicar prácticas de trabajo reproducible** (estructura de proyecto, `requirements.txt`, variables de entorno para credenciales y conexión segura).\n",
    "\n",
    "**Criterios de logro (nivelación):**\n",
    "- **Básico:** la app de Streamlit corre localmente y muestra una tabla consultada de PostgreSQL.  \n",
    "- **Intermedio:** ETL (DTS) carga datos limpios y la app provee al menos dos vistas (tabla + gráfico/simple filtro) con consultas parametrizadas.  \n",
    "- **Avanzado:** validaciones implementadas, índices/clave primaria definidos, parámetros configurables y README que documenta el pipeline y el esquema.\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-requisitos y conocimientos previos\n",
    "- Python intermedio; manejo de `pip`/`venv`.  \n",
    "- Conocimientos básicos de SQL (SELECT, CREATE TABLE, tipos).  \n",
    "- Lecturas previas sugeridas (opcional): documentación intro de Streamlit y de PostgreSQL.\n",
    "\n",
    "---\n",
    "\n",
    "## Materiales y recursos\n",
    "- **Software:** Python 3.10+, `streamlit`, `pandas`, **`SQLAlchemy`**, **`psycopg2-binary`** (o `psycopg`), `python-dotenv`.  \n",
    "- **Servicios:** Servidor **PostgreSQL** accesible para estudiantes (credenciales de práctica y base/schema por sección).  \n",
    "- **Datos de práctica:** CSV provisto por la cátedra (ej.: `ventas.csv` o `sensores.csv`, sin PII).  \n",
    "- **Infraestructura:** Computadores con proyector/pizarra; LMS para subir materiales; repositorio (Git) opcional.\n",
    "\n",
    "---\n",
    "\n",
    "## Guion temporal detallado (225 min)\n",
    "> Nota: para una versión de **180 min**, reducir los tiempos de práctica guiada (Bloques 4 y 5) en ~45 min total.\n",
    "\n",
    "### Bloque 1 — Apertura y activación (20 min)\n",
    "- **5’** Presentación de objetivos, criterios de logro y productos esperados.  \n",
    "- **10’** Activación: breve sondeo de experiencias previas (mentimeter o mano alzada) sobre web apps y SQL.  \n",
    "- **5’** Revisión de agenda y entregables (ticket de salida y mini-demo).\n",
    "\n",
    "### Bloque 2 — Marco conceptual y setup (40 min)\n",
    "- **10’** Mini-clase: arquitectura ligera con Streamlit; patrón ETL/DTS; riesgos (credenciales, PII, tipos, inyección SQL).  \n",
    "- **15’** Setup guiado: crear `venv` y `pip install -U streamlit pandas sqlalchemy psycopg2-binary python-dotenv`.  \n",
    "- **15’** Conexión a PostgreSQL: variables de entorno (`.env`), DSN/URI (p. ej., `postgresql+psycopg2://user:pass@host:port/db`), prueba de conexión y creación de **schema**.\n",
    "\n",
    "### Bloque 3 — DTS/ETL: de CSV a PostgreSQL (45 min)\n",
    "- **10’** Limpieza mínima con `pandas`: tipos, nulos, normalización de fechas/categorías.  \n",
    "- **20’** Definición de esquema y carga: `CREATE TABLE` (clave primaria, tipos correctos), `to_sql` con SQLAlchemy o `COPY`/`executemany`.  \n",
    "- **15’** Validaciones básicas: conteo de filas, chequeo de nulos, unicidad de clave, tipos correctos (ej.: `DATE`, `NUMERIC`).\n",
    "\n",
    "### Bloque 4 — App Streamlit inicial (60 min)\n",
    "- **15’** Estructura de proyecto y archivo `app.py`. Conexión con SQLAlchemy (`create_engine`) y `pandas.read_sql`.  \n",
    "- **25’** Interacción: filtros parametrizados (evitar inyección), agregaciones (`GROUP BY`) y gráfico simple (`st.line_chart`/`st.bar_chart`).  \n",
    "- **20’** Variables de entorno y configuración (`.env`, `.streamlit/secrets.toml` opcional); manejo de errores y timeouts.\n",
    "\n",
    "### Bloque 5 — Mejora, demo y cierre (60 min)\n",
    "- **20’** Mejora incremental: vistas o tabs; índices adicionales para rendimiento; caché selectiva (`st.cache_data`).  \n",
    "- **20’** Ticket de salida (individual): checklist de calidad + mini-demo ejecutable.  \n",
    "- **20’** Puesta en común y retroalimentación cruzada. Indicaciones de tarea autónoma (README + mejoras).\n",
    "\n",
    "**Tiempo total:** 225 min.\n",
    "\n",
    "---\n",
    "\n",
    "## Instrucciones paso a paso (docente y estudiantes)\n",
    "**Docente**  \n",
    "1. Verificar con antelación acceso a servidor PostgreSQL (roles, DB y schema por sección).  \n",
    "2. Proveer `requirements.txt`, plantilla de `.env.example` con: `PGHOST`, `PGPORT`, `PGUSER`, `PGPASSWORD`, `PGDATABASE` o `DATABASE_URL`.  \n",
    "3. Entregar DDL base sugerido (tabla(s) objetivo) y dataset CSV.  \n",
    "4. Demostrar una app mínima en vivo y promover adaptación por equipos.  \n",
    "5. Facilitar rúbrica breve y checklist de seguridad/datos.\n",
    "\n",
    "**Estudiantes**  \n",
    "1. Crear y activar entorno virtual; instalar dependencias.  \n",
    "2. Diseñar esquema mínimo (tipos y clave); cargar CSV transformado a PostgreSQL.  \n",
    "3. Construir `app.py` que ejecuta consultas **parametrizadas** y despliega tabla + 1 gráfico + 1 filtro.  \n",
    "4. Colocar credenciales en `.env` (no subirlas al repo).  \n",
    "5. Completar ticket de salida y preparar mini-demo.\n",
    "\n",
    "---\n",
    "\n",
    "## Estrategias de diferenciación e inclusión\n",
    "- **Apoyo escalonado:** parejas heterogéneas (peer tutoring) y snippets de SQL parametrizado.  \n",
    "- **Rutas alternativas:** dataset más pequeño para equipos con menor avance; dataset mayor para profundización.  \n",
    "- **Accesibilidad:** fuentes legibles, contraste en proyector; materiales con capturas y pasos numerados.  \n",
    "- **Evaluación flexible:** evidencias equivalentes (capturas, video corto de demo, o repo con README).\n",
    "\n",
    "---\n",
    "\n",
    "## Evidencias / Productos esperados\n",
    "- **Código ejecutable** (`app.py`, script ETL `load.py` o notebook) + `requirements.txt` + `.env.example`.  \n",
    "- **DDL del esquema** (`schema.sql`) con `CREATE TABLE` e índices.  \n",
    "- **Mini-demo** (video 1–2 min o en vivo).  \n",
    "- **Ticket de salida** con checklist de calidad (ver más abajo).  \n",
    "- **README** básico (estructura del proyecto, decisiones de DTS y modelo de datos).\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluación formativa y criterios de logro\n",
    "**Instrumentos:**  \n",
    "- **Ticket de salida (10 ptos):** checklist binario (sí/no) + 2 preguntas cortas de reflexión.  \n",
    "- **Observación de desempeño (10 ptos):** durante práctica guiada (configuración, carga y consulta SQL funcional).\n",
    "\n",
    "**Rúbrica breve (ejemplo, 20 ptos):**\n",
    "- Configuración y conexión segura (env, requirements) — **5 ptos**  \n",
    "- ETL/DTS correcto y validado (tipos, nulos, recuentos, clave/índices) — **7 ptos**  \n",
    "- App con tabla + gráfico + filtro (consultas parametrizadas) — **6 ptos**  \n",
    "- Documentación mínima (README, DDL) — **2 ptos**\n",
    "\n",
    "**Retroalimentación:** inmediata al finalizar Bloque 5 + comentarios escritos sobre el ticket.\n",
    "\n",
    "---\n",
    "\n",
    "## Carga total estimada\n",
    "- **Docencia directa:** 225 min  \n",
    "- **Trabajo autónomo posterior:** 60–90 min (mejoras + documentación)  \n",
    "- **Carga total de la sesión:** 285–315 min\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos y competencias del laboratorio (prácticas)\n",
    "- Conectar servicios (app ↔ base de datos) con **credenciales seguras** y consultas **parametrizadas**.  \n",
    "- Implementar **DTS** reproducible con registro de decisiones (data log).  \n",
    "- Operar herramientas de **visualización rápida** con Streamlit para explorar datos.\n",
    "\n",
    "---\n",
    "\n",
    "## Protocolo de seguridad y EPP\n",
    "- **Ergonómico y eléctrico:** cableado ordenado, evitar sobrecarga de enchufes.  \n",
    "- **Datos y privacidad:** no usar PII; **no exponer credenciales**; `.env` y mínimo de privilegios (rol app sin superusuario).  \n",
    "- **Red/SSL:** preferir conexiones cifradas; rotación de contraseñas de práctica.  \n",
    "- **EPP:** no aplica EPP físico específico; sí “EPP digital”: gestor de contraseñas y 2FA cuando aplique.\n",
    "\n",
    "---\n",
    "\n",
    "## Checklist de materiales y preparación\n",
    "- [ ] Python 3.10+ y permisos de instalación.  \n",
    "- [ ] `streamlit`, `pandas`, `sqlalchemy`, `psycopg2-binary` (o `psycopg`), `python-dotenv`.  \n",
    "- [ ] Servidor PostgreSQL accesible y credenciales por equipo.  \n",
    "- [ ] CSV de práctica validado (sin PII, con diccionario de datos).  \n",
    "- [ ] Plantillas: `requirements.txt`, `.env.example`, `schema.sql`, estructura de carpetas.  \n",
    "- [ ] Proyector y conexión a internet estable.\n",
    "\n",
    "---\n",
    "\n",
    "## Procedimiento experimental (paso a paso) con control de riesgos\n",
    "1. **Setup (venv)** — crear entorno virtual y activar. *Riesgo:* conflictos de versiones → *Mitigación:* `requirements.txt` fijo.  \n",
    "2. **Instalación** — `pip install -U streamlit pandas sqlalchemy psycopg2-binary python-dotenv`. *Riesgo:* errores de red → reintentos/control de mirroring.  \n",
    "3. **Credenciales** — crear `.env` con `DATABASE_URL` o variables `PG*`; probar conexión con SQLAlchemy. *Riesgo:* filtración de secretos → `.gitignore` y revisión visual.  \n",
    "4. **Esquema** — definir DDL (`CREATE TABLE ...`) con tipos (`INTEGER`, `DATE`, `TIMESTAMP`, `NUMERIC`, `TEXT`) y **clave primaria**. *Riesgo:* malos tipos → validación previa en `pandas`.  \n",
    "5. **Extracción** — leer CSV (`pandas.read_csv`) con `dtype` y `parse_dates`. *Riesgo:* tipos erróneos → esquema explícito.  \n",
    "6. **Transformación** — limpieza (nulos, categorías, fechas). *Riesgo:* pérdida de información → registrar decisiones en README.  \n",
    "7. **Carga** — `df.to_sql(..., if_exists='append')` o `COPY`/`executemany` para mejor rendimiento. *Riesgo:* duplicados → restricciones `UNIQUE`/índices y control por clave natural.  \n",
    "8. **App** — `read_sql` con **consultas parametrizadas**; `st.dataframe`, `st.bar_chart`; filtros con `st.selectbox/multiselect`. *Riesgo:* inyección SQL → parámetros, no concatenar strings.  \n",
    "9. **Rendimiento** — crear índices en columnas de filtro; limitar filas (`LIMIT`) y paginar. *Riesgo:* tiempos largos → explicar trade-offs.  \n",
    "10. **Pruebas** — `streamlit run app.py`. *Riesgo:* puertos ocupados → cambiar puerto (`--server.port`).  \n",
    "11. **Documentación** — README y capturas. *Riesgo:* falta de trazabilidad → subir versión final al LMS.\n",
    "\n",
    "---\n",
    "\n",
    "## Criterios de calidad de datos e integridad académica\n",
    "- **Esquema documentado** (nombres, tipos, nulos permitidos, claves).  \n",
    "- **Validaciones automáticas**: conteos, rangos, duplicados, tipos esperados.  \n",
    "- **Restricciones en BD**: `NOT NULL`, `CHECK`, `UNIQUE`, PK/FK cuando aplique.  \n",
    "- **Reproducibilidad**: script/Notebook + `requirements.txt` + `schema.sql`.  \n",
    "- **Integridad académica**: citar fuentes de datos, no copiar código sin atribución, uso correcto de licencias.\n",
    "\n",
    "---\n",
    "\n",
    "## Plan de contingencias\n",
    "- **BD no disponible:** conmutar a **SQLite** local para la demo (mismos pandas/SQLAlchemy), y reintentar carga a PostgreSQL luego.  \n",
    "- **Errores de instalación:** proveer entorno portable preconfigurado o usar Codespaces alternativo.  \n",
    "- **Tiempo insuficiente (ajuste a 180 min):** omitir segunda vista y simplificar validaciones al chequeo básico.\n",
    "\n",
    "---\n",
    "\n",
    "## Ticket de salida (plantilla)\n",
    "- [ ] App en ejecución local con tabla + gráfico + filtro (consultas parametrizadas).  \n",
    "- [ ] ETL ejecutado: filas cargadas = ____ ; duplicados eliminados = ____.  \n",
    "- [ ] `.env` configurado y excluido del repo.  \n",
    "- [ ] README con decisiones de limpieza, DDL y cómo ejecutar la app.\n",
    "\n",
    "---\n",
    "\n",
    "## Tarea para casa \n",
    "- Añadir **paginación** o **búsqueda** en la tabla.  \n",
    "- Incorporar **gráfico alternativo** y **descarga** de datos filtrados.  \n",
    "- Desplegar en **Streamlit Community Cloud** / servicio institucional con variables de entorno seguras.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
